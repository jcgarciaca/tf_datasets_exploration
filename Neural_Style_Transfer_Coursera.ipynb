{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Style Transfer Coursera.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLgfXF5mQreK"
      },
      "source": [
        "# Neural Style Transfer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INKwFqeDHkyI"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras import backend as K\n",
        "from imageio import mimsave\n",
        "from IPython.display import display as display_fn\n",
        "from IPython.display import Image, clear_output\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4Il7H-lH5U_"
      },
      "source": [
        "def tensor_to_image(tensor):\n",
        "  '''converts a tensor to an image'''\n",
        "  tensor_shape = tf.shape(tensor)\n",
        "  number_elem_shape = tf.shape(tensor_shape)\n",
        "  if number_elem_shape > 3:\n",
        "    assert tensor_shape[0] == 1\n",
        "    tensor = tensor[0]\n",
        "  return tf.keras.preprocessing.image.array_to_img(tensor)\n",
        "\n",
        "def load_img(path_to_img):\n",
        "  '''loads an image as a tensor and scales it to 512 pixels'''\n",
        "  max_dim = 512\n",
        "  image = tf.io.read_file(path_to_img)\n",
        "  image = tf.image.decode_jpeg(image)\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "\n",
        "  shape = tf.shape(image)[:-1]\n",
        "  shape = tf.cast(tf.shape(image)[:-1], tf.float32)\n",
        "  long_dim = max(shape)\n",
        "  scale = max_dim / long_dim\n",
        "\n",
        "  new_shape = tf.cast(shape * scale, tf.int32)\n",
        "\n",
        "  image = tf.image.resize(image, new_shape)\n",
        "  image = image[tf.newaxis, :]\n",
        "  image = tf.image.convert_image_dtype(image, tf.uint8)\n",
        "  return image\n",
        "\n",
        "def load_images(content_path, style_path):\n",
        "  '''loads the content and path images as tensors'''\n",
        "  content_image = load_img('{}'.format(content_path))\n",
        "  style_image = load_img('{}'.format(style_path))\n",
        "  return content_image, style_image\n",
        "\n",
        "def imshow(image, title=None):\n",
        "  '''displays an image with corresponding title'''\n",
        "  if len(image.shape) > 3:\n",
        "    image = tf.squeeze(image, axis=0)\n",
        "\n",
        "  plt.imshow(image)\n",
        "  if title:\n",
        "    plt.title(title)\n",
        "\n",
        "def show_images_with_objects(images, titles=[]):\n",
        "  '''displays a row of images with corresponding titles'''\n",
        "  if len(images) != len(titles):\n",
        "    return\n",
        "  \n",
        "  plt.figure(figsize=(20, 12))\n",
        "  for idx, (image, title) in enumerate(zip(images, titles)):\n",
        "    plt.subplot(1, len(images), idx + 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    imshow(image, title)\n",
        "\n",
        "def display_gif(gif_path):\n",
        "  '''displays the generated images as an animated gif'''\n",
        "  with open(gif_path, 'rb') as f:\n",
        "    display_fn(Image(data=f.read(), format='png'))\n",
        "\n",
        "def create_gif(gif_path, images):\n",
        "  '''create animation of generated images'''\n",
        "  mimsave(gif_path, images, fps=1)\n",
        "  return gif_path\n",
        "\n",
        "def clip_image_values(image, min_value=0.0, max_value=255.0):\n",
        "  '''clips the image pixel values by the given min and max'''\n",
        "  return tf.clip_by_value(image, clip_value_min=min_value, clip_value_max=max_value)\n",
        "\n",
        "def preprocess_image(image):\n",
        "  '''centers the image pixel values of a given image to use with VGG-19'''\n",
        "  image = tf.cast(image, dtype=tf.float32)\n",
        "  image = tf.keras.applications.vgg19.preprocess_input(image)\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nc1QnwxQpDh"
      },
      "source": [
        "## Download images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vQ-YuUvQmsd"
      },
      "source": [
        "IMAGE_DIR = 'images'\n",
        "\n",
        "!mkdir {IMAGE_DIR}\n",
        "\n",
        "# download images to the directory you just created\n",
        "!wget -q -O ./images/cafe.jpg https://cdn.pixabay.com/photo/2018/07/14/15/27/cafe-3537801_1280.jpg\n",
        "!wget -q -O ./images/swan.jpg https://cdn.pixabay.com/photo/2017/02/28/23/00/swan-2107052_1280.jpg\n",
        "!wget -q -O ./images/tnj.jpg https://i.dawn.com/large/2019/10/5db6a03a4c7e3.jpg\n",
        "!wget -q -O ./images/rudolph.jpg https://cdn.pixabay.com/photo/2015/09/22/12/21/rudolph-951494_1280.jpg\n",
        "!wget -q -O ./images/dynamite.jpg https://cdn.pixabay.com/photo/2015/10/13/02/59/animals-985500_1280.jpg\n",
        "!wget -q -O ./images/painting.jpg https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg\n",
        "\n",
        "print(\"image files you can choose from: \")\n",
        "!ls images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBYB97A0RBO5"
      },
      "source": [
        "# set default images\n",
        "content_path = f'{IMAGE_DIR}/swan.jpg'\n",
        "style_path = f'{IMAGE_DIR}/painting.jpg'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLsl5MR8RUO9"
      },
      "source": [
        "# display content and style images\n",
        "content_image, style_image = load_images(content_path, style_path)\n",
        "show_images_with_objects([content_image, style_image], \n",
        "                         titles=[f'content image: {content_path}',\n",
        "                                 f'style image: {style_path}'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MsyORws3j_-"
      },
      "source": [
        "print(np.max(content_image), content_image.shape)\n",
        "print(np.max(style_image), style_image.shape)\n",
        "print(np.max(preprocess_image(content_image)), np.min(preprocess_image(content_image)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8LQbUGMq6qv"
      },
      "source": [
        "## Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zzMxluDq30B"
      },
      "source": [
        "# clear session to make layer naming consistent when re-running this cell\n",
        "K.clear_session()\n",
        "\n",
        "tmp_vgg = tf.keras.applications.vgg19.VGG19()\n",
        "tmp_vgg.summary()\n",
        "\n",
        "del tmp_vgg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESWd2TThrUXQ"
      },
      "source": [
        "style_layers = ['block{}_conv1'.format(x) for x in range(1, 6)]\n",
        "content_layers = ['block5_conv2']\n",
        "output_layers = style_layers + content_layers\n",
        "NUM_CONTENT_LAYERS = len(content_layers)\n",
        "NUM_STYLE_LAYERS = len(style_layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o12_uaDRry-y"
      },
      "source": [
        "def vgg_model(layer_names):\n",
        "  \"\"\" Creates a vgg model that outputs the style and content layer activations.\n",
        "  \n",
        "  Args:\n",
        "    layer_names: a list of strings, representing the names of the desired content and style layers\n",
        "    \n",
        "  Returns:\n",
        "    A model that takes the regular vgg19 input and outputs just the content and style layers.\n",
        "  \n",
        "  \"\"\"\n",
        "  vgg = tf.keras.applications.vgg19.VGG19(include_top=False,\n",
        "                                          weights='imagenet')\n",
        "  vgg.trainable = False\n",
        "  outputs = [vgg.get_layer(name).output for name in layer_names]\n",
        "\n",
        "  model = tf.keras.Model(inputs=vgg.input, outputs=outputs)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVhMq1oIs5ul"
      },
      "source": [
        "# clear session to make layer naming consistent if re-running the cell\n",
        "K.clear_session()\n",
        "\n",
        "vgg = vgg_model(output_layers)\n",
        "vgg.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKfDi5Drtijo"
      },
      "source": [
        "## Define the loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzG79n24tL85"
      },
      "source": [
        "def get_style_loss(features, targets):\n",
        "  \"\"\"Expects two images of dimension h, w, c\n",
        "  \n",
        "  Args:\n",
        "    features: tensor with shape: (height, width, channels)\n",
        "    targets: tensor with shape: (height, width, channels)\n",
        "\n",
        "  Returns:\n",
        "    style loss (scalar)\n",
        "  \"\"\"\n",
        "  return tf.reduce_mean(tf.square(features - targets))\n",
        "\n",
        "def get_content_loss(features, targets):\n",
        "  \"\"\"Expects two images of dimension h, w, c\n",
        "  \n",
        "  Args:\n",
        "    features: tensor with shape: (height, width, channels)\n",
        "    targets: tensor with shape: (height, width, channels)\n",
        "  \n",
        "  Returns:\n",
        "    content loss (scalar)\n",
        "  \"\"\"\n",
        "  return 0.5 * tf.reduce_sum(tf.square(features - targets))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19fyKbPbe4Mg"
      },
      "source": [
        "### Calculate Gram Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTFvijpaeLIM"
      },
      "source": [
        "def gram_matrix(input_tensor):\n",
        "  \"\"\" Calculates the gram matrix and divides by the number of locations\n",
        "  Args:\n",
        "    input_tensor: tensor of shape (batch, height, width, channels)\n",
        "    \n",
        "  Returns:\n",
        "    scaled_gram: gram matrix divided by the number of locations\n",
        "  \"\"\"\n",
        "  gram = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
        "  input_shape = tf.shape(input_tensor)\n",
        "  height = input_shape[1]\n",
        "  width = input_shape[2]\n",
        "\n",
        "  num_locations = tf.cast(height * width, tf.float32)\n",
        "  scaled_gram = gram / num_locations\n",
        "  return scaled_gram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7TjZwsOfq3Y"
      },
      "source": [
        "### Get style image features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UjxIx_Vfoy-"
      },
      "source": [
        "tmp_layer_list = [layer.output for layer in vgg.layers]\n",
        "tmp_layer_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faHbdiTuf8Uw"
      },
      "source": [
        "def get_style_image_features(image):\n",
        "  \"\"\" Get the style image features\n",
        "  \n",
        "  Args:\n",
        "    image: an input image\n",
        "    \n",
        "  Returns:\n",
        "    gram_style_features: the style features as gram matrices\n",
        "  \"\"\"\n",
        "  preprocessed_style_image = preprocess_image(image)\n",
        "  outputs = vgg(preprocessed_style_image)\n",
        "  style_outputs = outputs[:NUM_STYLE_LAYERS]\n",
        "  gram_style_features = [gram_matrix(style_layer) for style_layer in style_outputs]\n",
        "  return gram_style_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNRcXJ04g5d9"
      },
      "source": [
        "### Get content image features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnYNYIUJg3tS"
      },
      "source": [
        "def get_content_image_features(image):\n",
        "  \"\"\" Get the content image features\n",
        "  \n",
        "  Args:\n",
        "    image: an input image\n",
        "    \n",
        "  Returns:\n",
        "    content_outputs: the content features of the image\n",
        "  \"\"\"\n",
        "  preprocessed_content_image = preprocess_image(image)\n",
        "  outputs = vgg(preprocessed_content_image)\n",
        "  content_outputs = outputs[NUM_STYLE_LAYERS:]\n",
        "  return content_outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln6nVp_Shun6"
      },
      "source": [
        "### Calculate style and content loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OziEAjrHht9C"
      },
      "source": [
        "def get_style_content_loss(style_targets, style_outputs, content_targets, content_outputs, style_weight, content_weight):\n",
        "  \"\"\" Combine the style and content loss\n",
        "  Args:\n",
        "    style_targets: style features of the style image\n",
        "    style_outputs: style features of the generated image\n",
        "    content_targets: content features of the content image\n",
        "    content_outputs: content features of the generated image\n",
        "    style_weight: weight given to the style loss\n",
        "    content_weight: weight given to the content loss\n",
        "\n",
        "  Returns:\n",
        "    total_loss: the combined style and content loss\n",
        "  \"\"\"\n",
        "  style_loss = tf.add_n([get_style_loss(style_output, style_target) for style_output, style_target in zip(style_outputs, style_targets)])\n",
        "  content_loss = tf.add_n([get_content_loss(content_output, content_target) for content_output, content_target in zip(content_outputs, content_targets)])\n",
        "  style_loss = style_loss * style_weight / NUM_STYLE_LAYERS\n",
        "  content_loss = content_loss * content_weight / NUM_CONTENT_LAYERS\n",
        "  total_loss = style_loss + content_loss\n",
        "  return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Se4jxNnjSEE"
      },
      "source": [
        "## Generate the stylized image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKFVzii-jPS8"
      },
      "source": [
        "def calculate_gradients(image, style_targets, content_targets, style_weight, content_weight, var_weight):\n",
        "  \"\"\" Calculate the gradients of the loss with respect to the generated image\n",
        "  Args:\n",
        "    image: generated image\n",
        "    style_targets: style features of the style image\n",
        "    content_targets: content features of the content image\n",
        "    style_weight: weight given to the style loss\n",
        "    content_weight: weight given to the content loss\n",
        "    var_weight: weight given to the total variation loss\n",
        "  \n",
        "  Returns:\n",
        "    gradients: gradients of the loss with respect to the input image\n",
        "  \"\"\"\n",
        "  with tf.GradientTape() as tape:\n",
        "    style_features = get_style_image_features(image)\n",
        "    content_features = get_content_image_features(image)\n",
        "    loss = get_style_content_loss(style_targets, style_features, content_targets, content_features, style_weight, content_weight)\n",
        "  gradients = tape.gradient(loss, image)\n",
        "  return gradients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_4IE8o5kZ33"
      },
      "source": [
        "### Update the image with style"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLUFGvOlkWW_"
      },
      "source": [
        "def update_image_with_style(image, style_targets, content_targets, style_weight, var_weight, content_weight, optimizer):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    image: generated image\n",
        "    style_targets: style features of the style image\n",
        "    content_targets: content features of the content image\n",
        "    style_weight: weight given to the style loss\n",
        "    content_weight: weight given to the content loss\n",
        "    var_weight: weight given to the total variation loss\n",
        "    optimizer: optimizer for updating the input image\n",
        "  \"\"\"\n",
        "  gradients = calculate_gradients(image, style_targets, content_targets, style_weight, content_weight, var_weight)\n",
        "  optimizer.apply_gradients([(gradients, image)])\n",
        "  image.assign(clip_image_values(image, min_value=0.0, max_value=255.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzUIQ1QGlE8w"
      },
      "source": [
        "## Style transfer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3TH1gCllEKJ"
      },
      "source": [
        "def fit_style_transfer(style_image, content_image, style_weight=1e-2, content_weight=1e-4, var_weight=0, optimizer='adam', epochs=1, steps_per_epoch=1):\n",
        "  \"\"\" Performs neural style transfer.\n",
        "  Args:\n",
        "    style_image: image to get style features from\n",
        "    content_image: image to stylize \n",
        "    style_targets: style features of the style image\n",
        "    content_targets: content features of the content image\n",
        "    style_weight: weight given to the style loss\n",
        "    content_weight: weight given to the content loss\n",
        "    var_weight: weight given to the total variation loss\n",
        "    optimizer: optimizer for updating the input image\n",
        "    epochs: number of epochs\n",
        "    steps_per_epoch = steps per epoch\n",
        "  \n",
        "  Returns:\n",
        "    generated_image: generated image at final epoch\n",
        "    images: collection of generated images per epoch  \n",
        "  \"\"\"\n",
        "  images = []\n",
        "  step = 0\n",
        "  style_targets = get_style_image_features(style_image)\n",
        "  content_targets = get_content_image_features(content_image)\n",
        "  \n",
        "  generated_image = tf.cast(content_image, dtype=tf.float32)\n",
        "  generated_image = tf.Variable(generated_image)\n",
        "\n",
        "  images.append(content_image)\n",
        "\n",
        "  for n in range(epochs):\n",
        "    for m in range(steps_per_epoch):\n",
        "      step += 1\n",
        "\n",
        "      update_image_with_style(generated_image, style_targets, content_targets, style_weight, var_weight, content_weight, optimizer)\n",
        "      print(\".\", end='')\n",
        "\n",
        "      if (m + 1) % 10  == 0:\n",
        "        images.append(generated_image)\n",
        "      \n",
        "    clear_output(wait=True)\n",
        "    display_image = tensor_to_image(generated_image)\n",
        "    display_fn(display_image)\n",
        "\n",
        "    images.append(generated_image)\n",
        "    print(\"Train step: {}\".format(step))\n",
        "  \n",
        "  generated_image = tf.cast(generated_image, tf.uint8)\n",
        "  return generated_image, images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPs6DW1rp7sw"
      },
      "source": [
        "style_weight = 1e-4\n",
        "content_weight = 1e-32\n",
        "\n",
        "adam = tf.optimizers.Adam(\n",
        "    tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate=30.0, decay_steps=100, decay_rate=0.80\n",
        "    )\n",
        ")\n",
        "\n",
        "stylized_image, display_images = fit_style_transfer(style_image, content_image, style_weight, content_weight, var_weight=0, \n",
        "                                                    optimizer=adam, epochs=10, steps_per_epoch=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZOtZemPq6H1"
      },
      "source": [
        "GIF_PATH = 'style_transfer.gif'\n",
        "gif_images = [np.squeeze(image.numpy().astype(np.uint8), axis=0) for image in display_images]\n",
        "gif_path = create_gif(GIF_PATH, gif_images)\n",
        "display_gif(gif_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f8VYprk4V6d"
      },
      "source": [
        "## Total variation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QerZMHTyryGR"
      },
      "source": [
        "# plot utilities\n",
        "\n",
        "def high_pass_x_y(image):\n",
        "  x_var = image[:, :, 1:, :] - image[:, :, :-1, :]\n",
        "  y_var = image[:, 1:, :, :] - image[:, :-1, :, :]\n",
        "  return x_var, y_var\n",
        "\n",
        "def plot_deltas_for_single_image(x_deltas, y_deltas, name=\"Original\", row=1):\n",
        "  plt.figure(figsize=(14,10))\n",
        "  plt.subplot(row,2,1)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "\n",
        "  clipped_y_deltas = clip_image_values(2*y_deltas+0.5, min_value=0.0, max_value=1.0)\n",
        "  imshow(clipped_y_deltas, \"Horizontal Deltas: {}\".format(name))\n",
        "\n",
        "  plt.subplot(row,2,2)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "  \n",
        "  clipped_x_deltas = clip_image_values(2*x_deltas+0.5, min_value=0.0, max_value=1.0)\n",
        "  imshow(clipped_x_deltas, \"Vertical Deltas: {}\".format(name))\n",
        "\n",
        "\n",
        "def plot_deltas(original_image_deltas, stylized_image_deltas):\n",
        "  orig_x_deltas, orig_y_deltas = original_image_deltas\n",
        "  \n",
        "  stylized_x_deltas, stylized_y_deltas = stylized_image_deltas\n",
        "\n",
        "  plot_deltas_for_single_image(orig_x_deltas, orig_y_deltas, name=\"Original\")\n",
        "  plot_deltas_for_single_image(stylized_x_deltas, stylized_y_deltas, name=\"Stylized Image\", row=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FyJppXV4_9F"
      },
      "source": [
        "# Display the frequency variations\n",
        "\n",
        "original_x_deltas, original_y_deltas = high_pass_x_y(\n",
        "    tf.image.convert_image_dtype(content_image, dtype=tf.float32))\n",
        "\n",
        "stylized_image_x_deltas, stylized_image_y_deltas = high_pass_x_y(\n",
        "    tf.image.convert_image_dtype(stylized_image, dtype=tf.float32))\n",
        "\n",
        "plot_deltas((original_x_deltas, original_y_deltas), (stylized_image_x_deltas, stylized_image_y_deltas))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ggl9xMEP5DTT"
      },
      "source": [
        "def calculate_gradients(image, style_targets, content_targets, \n",
        "                        style_weight, content_weight, var_weight):\n",
        "  \"\"\" Calculate the gradients of the loss with respect to the generated image\n",
        "  Args:\n",
        "    image: generated image\n",
        "    style_targets: style features of the style image\n",
        "    content_targets: content features of the content image\n",
        "    style_weight: weight given to the style loss\n",
        "    content_weight: weight given to the content loss\n",
        "    var_weight: weight given to the total variation loss\n",
        "  \n",
        "  Returns:\n",
        "    gradients: gradients of the loss with respect to the input image\n",
        "  \"\"\"\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    style_features = get_style_image_features(image)\n",
        "    content_features = get_content_image_features(image)\n",
        "    loss = get_style_content_loss(style_targets, style_features, content_targets, content_features, style_weight, content_weight)\n",
        "    loss += var_weight * tf.image.total_variation(image)\n",
        "  gradients = tape.gradient(loss, image)\n",
        "  return gradients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXJK73er5wKt"
      },
      "source": [
        "style_weight =  1e-4\n",
        "content_weight = 1e-32\n",
        "var_weight = 1e-2\n",
        "\n",
        "adam = tf.optimizers.Adam(\n",
        "    tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate=30.0, decay_steps=100, decay_rate=0.90\n",
        "    )\n",
        ")\n",
        "\n",
        "stylized_image_reg, display_images_reg = fit_style_transfer(style_image=style_image, content_image=content_image, \n",
        "                                                    style_weight=style_weight, content_weight=content_weight,\n",
        "                                                    var_weight=var_weight, optimizer=adam, epochs=10, steps_per_epoch=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFZQHQI35ybI"
      },
      "source": [
        "# Display GIF\n",
        "GIF_PATH = 'style_transfer_reg.gif'\n",
        "gif_images_reg = [np.squeeze(image.numpy().astype(np.uint8), axis=0) for image in display_images_reg]\n",
        "gif_path_reg = create_gif(GIF_PATH, gif_images_reg)\n",
        "display_gif(gif_path_reg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEuIQlWT552M"
      },
      "source": [
        "# Display Frequency Variations\n",
        "\n",
        "original_x_deltas, original_y_deltas = high_pass_x_y(\n",
        "    tf.image.convert_image_dtype(content_image, dtype=tf.float32))\n",
        "\n",
        "stylized_image_reg_x_deltas, stylized_image_reg_y_deltas = high_pass_x_y(\n",
        "    tf.image.convert_image_dtype(stylized_image_reg, dtype=tf.float32))\n",
        "\n",
        "plot_deltas((original_x_deltas, original_y_deltas), (stylized_image_reg_x_deltas, stylized_image_reg_y_deltas))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SHWi7vE59tH"
      },
      "source": [
        "show_images_with_objects([style_image, content_image, stylized_image], titles=['Style Image', 'Content Image', 'Stylized Image'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwpSxUKK6HBl"
      },
      "source": [
        "show_images_with_objects([style_image, content_image, stylized_image_reg], titles=['Style Image', 'Content Image', 'Stylized Image with Regularization'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia5IjHKN6HTD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}